# Data Engineering E-commerce Project

Este projeto Ã© um estudo em **Engenharia de Dados**, utilizando dados pÃºblicos de e-commerce (Kaggle Olist Dataset).
O objetivo foi construir uma **plataforma de dados** completa, passando por ingestÃ£o, modelagem, pipelines e anÃ¡lises, utilizando **Postgres, Python, Azure e PySpark**.

---

## ğŸš€ Objetivos do Projeto

* Praticar **ETL/ELT** com grandes volumes de dados.
* Trabalhar com **SQL avanÃ§ado** e **Postgres**.
* Desenvolver pipelines em **Python**.
* Evoluir para integraÃ§Ã£o com **Azure (Data Factory, Databricks, Synapse)**.
* Construir um **Lakehouse** com PySpark.

---

## ğŸ“‚ Estrutura de Pastas

```
data-engineering-ecommerce/
â”‚â”€â”€ data/               # Datasets brutos (CSV do Kaggle)
â”‚â”€â”€ docs/               # DocumentaÃ§Ã£o, diagramas, consultas SQL
â”‚   â”œâ”€â”€ diagrama_er.sql
â”‚   â””â”€â”€ consultas.sql
â”‚â”€â”€ src/                # Scripts Python de ingestÃ£o
â”‚   â””â”€â”€ ingest.py
â”‚â”€â”€ docker/             # ConfiguraÃ§Ã£o do Docker
â”‚   â””â”€â”€ docker-compose.yml
â”‚   â””â”€â”€ table.sql       # CriaÃ§Ã£o de schema e tabelas no Postgres
â”‚â”€â”€ requirements.txt    # DependÃªncias do projeto
â”‚â”€â”€ README.md           # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ³ Etapa 1 â€“ Banco de Dados (Postgres)

O Postgres foi configurado usando Docker Compose (`docker/docker-compose.yml`).
O schema e as tabelas foram criados no arquivo `docker/table.sql`.

---

## ğŸ Etapa 2 â€“ Ambiente Python

O projeto pode ser executado em um **ambiente virtual** para garantir dependÃªncias isoladas.

---

## ğŸ“Š Etapa 3 â€“ IngestÃ£o de Dados

A ingestÃ£o de todos os CSVs Ã© realizada pelo script `src/ingest.py`.
Ele lÃª os arquivos de dados e carrega nas tabelas do schema `raw` no Postgres.

---

## ğŸ“ Etapa 4 â€“ Diagrama ER

O diagrama ER do projeto estÃ¡ disponÃ­vel em `docs/diagrama_er.sql`.
O diagrama exportado foi salvo em `docs/er_diagram.png`.

---

## ğŸ” Etapa 5 â€“ Consultas Exploratorias

As consultas exploratÃ³rias utilizadas durante o projeto estÃ£o disponÃ­veis em `docs/consultas.sql`.

---

## ğŸ”® PrÃ³ximos Passos

* CriaÃ§Ã£o de pipelines incrementais para ingestÃ£o.
* TransformaÃ§Ãµes e modelagem para camada **curated**.
* IntegraÃ§Ã£o com **Azure Data Factory, Databricks e Synapse**.
* ConstruÃ§Ã£o de **Lakehouse com PySpark**.
* Desenvolvimento de dashboards ou anÃ¡lises exploratÃ³rias.

---

ğŸ“Œ *Este projeto serÃ¡ publicado passo a passo no LinkedIn como portfÃ³lio de Engenharia de Dados.*


